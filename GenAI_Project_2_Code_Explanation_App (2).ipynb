{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ILymDJuoMe",
        "outputId": "82bf4b24-5eaf-48be-bdb8-b2f7e189e0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os"
      ],
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key='your_api_key')"
      ],
      "metadata": {
        "id": "ZrLD58wovyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(palm.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfMIig2HO8iV",
        "outputId": "24c886dd-ebae-45bf-d455-ffec021a86a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "934ccfb3-e522-4cdd-9f76-e9395c951258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "008c4ff0-9923-4003-d822-8035b6e8bebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Chatbot**"
      ],
      "metadata": {
        "id": "IRNBhUFz6m3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Your Input Text\n",
        "prompt1 = \"Why is the Sky blue? Provide answers in bullain format in clear and concise maner?\"\n",
        "Prompt2 = \"Why Generative AI growing too much?\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt1,\n",
        "    temperature = 1,\n",
        "    # temperature =0 >> more deterministic results // temperature = 1 >> more randomness\n",
        "    max_output_tokens = 500\n",
        ")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "8pmhnMMA6p62",
        "outputId": "fc92dd7e-a620-4e85-b0f2-1b96b5d08e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Why is the sky blue?**\n",
            "\n",
            "* **Rayleigh scattering:** The shorter wavelengths of light (blue) are scattered more by the molecules in the atmosphere than the longer wavelengths (red). This makes the sky appear blue during the day.\n",
            "* **Tyndall scattering:** The scattering of light by small particles (such as dust and water droplets) also contributes to the blue color of the sky.\n",
            "* **Absorption by ozone:** Ozone in the stratosphere absorbs some of the sun's ultraviolet radiation, which makes the sky appear blue at higher altitudes.\n",
            "\n",
            "[Image of the sky at different altitudes, showing how the color changes from blue to dark blue to black](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Color_of_sky_at_different_altitudes.jpg/220px-Color_of_sky_at_different_altitudes.jpg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Your Input Text\n",
        "#prompt1 = \"Why is the Sky blue?\"\n",
        "prompt2 = \"Why Generative AI growing too much? Provide answers in bullain format in clear and concise maner?\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt2,\n",
        "    temperature = 0.9,\n",
        "    # temperature =0 >> more deterministic results // temperature = 1 >> more randomness\n",
        "    max_output_tokens = 500\n",
        ")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "k86thLvS7qdt",
        "outputId": "d8214037-927d-43d8-8bf3-5f34b4a7b94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Benefits of Generative AI**\n",
            "\n",
            "* **Creates new content:** Generative AI can be used to create new text, images, videos, and music. This can be used for a variety of purposes, such as generating marketing materials, creating art, or composing music.\n",
            "* **Personalizes experiences:** Generative AI can be used to personalize experiences for users. For example, it can be used to generate personalized recommendations, create customized content, or adapt user interfaces.\n",
            "* **Automates tasks:** Generative AI can be used to automate tasks that are currently done by humans. This can free up human workers to focus on other tasks, or it can help businesses to operate more efficiently.\n",
            "\n",
            "**Drawbacks of Generative AI**\n",
            "\n",
            "* **Potential for bias:** Generative AI models can be biased, either intentionally or unintentionally. This can lead to problems such as generating inaccurate or harmful content, or discriminating against certain groups of people.\n",
            "* **Risks to privacy:** Generative AI models can be used to create fake content, such as deepfakes or fake news. This can be used to deceive people or spread misinformation.\n",
            "* **Challenges to regulation:** Generative AI is a new technology, and there are few regulations in place to govern its use. This can lead to problems such as harmful content being generated or people's privacy being violated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  completion = palm.generate_text(\n",
        "      model=model,\n",
        "      prompt=prompt,\n",
        "      temperature=1,\n",
        "      # The maximum length of the response\n",
        "      max_output_tokens=600\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ],
      "metadata": {
        "id": "WyADRKKF-Sig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Prompting**"
      ],
      "metadata": {
        "id": "XEab6Q9_9MVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "x = [1,2,3,4,5]\n",
        "y = [i*i for i in x if i%2==0]\n",
        "print(y)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FOSv-jqL9Prh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "your Task is to act as a Python Code Explainer.\n",
        "I'll give you Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Breakdown the explanation as many steps as possible.\n",
        "Provide 3 testcase cases by the corresponding code snippet.\n",
        "Share intermidiate checkpoints & checkpoints in the results.\n",
        "Also, compute the final output of the code.\n",
        "Finally, interact and encourage\n",
        "Code Snippet is shared below, delimited with triple backticks.\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuE4xLPj9jQW",
        "outputId": "31ac7bb0-1f3d-4615-8675-6f768b4f8c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "your Task is to act as a Python Code Explainer.\n",
            "I'll give you Code Snippet.\n",
            "Your Job is to explain the Code Snippet step-by-step.\n",
            "Breakdown the explanation as many steps as possible.\n",
            "Provide 3 testcase cases by the corresponding code snippet.\n",
            "Share intermidiate checkpoints & checkpoints in the results.\n",
            "Also, compute the final output of the code.\n",
            "Finally, interact and encourage\n",
            "Code Snippet is shared below, delimited with triple backticks.\n",
            "```\n",
            "\n",
            "x = [1,2,3,4,5]\n",
            "y = [i*i for i in x if i%2==0]\n",
            "print(y)\n",
            "\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "4XjOGzqv-7NJ",
        "outputId": "d71d54c4-9ec8-4787-cfd3-b6df6b84015e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Code Explanation:**\n",
            "\n",
            "The code snippet is a Python program that takes a list of numbers as input and outputs a new list that contains the squares of the even numbers in the input list.\n",
            "\n",
            "The program starts by creating a variable called `x` and assigning it the value of the input list.\n",
            "\n",
            "```\n",
            "x = [1,2,3,4,5]\n",
            "```\n",
            "\n",
            "The next line of code creates a variable called `y` and assigns it the value of a list comprehension that iterates over the elements of `x` and only includes the elements that are even. The `*` operator in the list comprehension is used to square each element of the list.\n",
            "\n",
            "```\n",
            "y = [i*i for i in x if i%2==0]\n",
            "```\n",
            "\n",
            "The final line of code prints the contents of the `y` list.\n",
            "\n",
            "```\n",
            "print(y)\n",
            "```\n",
            "\n",
            "**Test Cases:**\n",
            "\n",
            "| Test Case | Input | Output |\n",
            "|---|---|---|\n",
            "| Test Case 1 | `[1,2,3,4,5]` | `[4,16]` |\n",
            "| Test Case 2 | `[-1,-2,-3,-4,-5]` | `[4,16]` |\n",
            "| Test Case 3 | `[0]` | `[0]` |\n",
            "\n",
            "**Intermidiate Checkpoints:**\n",
            "\n",
            "The first intermidiate checkpoint is the creation of the `x` list. This list contains the numbers 1, 2, 3, 4, and 5.\n",
            "\n",
            "```\n",
            "x = [1,2,3,4,5]\n",
            "```\n",
            "\n",
            "The second intermidiate checkpoint is the creation of the `y` list. This list contains the squares of the even numbers in the `x` list. The even numbers in the `x` list are 2 and 4, so the `y` list contains the values 4 and 16.\n",
            "\n",
            "```\n",
            "y = [i*i for i in x if i%2==0]\n",
            "```\n",
            "\n",
            "The final intermidiate checkpoint is the printing of the `y` list. The output of the program is the following list:\n",
            "\n",
            "```\n",
            "[4,16]\n",
            "```\n",
            "\n",
            "**Final Output:**\n",
            "\n",
            "The final output of the program is the list `[4,16]`. This list contains the squares of the even numbers in the input list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEwKUkI5ETeH",
        "outputId": "d52a51be-7137-4919-d97b-49b35adf1e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LowHighLow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R5q8x-uhD0MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to act as a Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your job is to explain the Code Snippet Step-by-Step.\n",
        "Also, compute the final output of the code.\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA4cwQv-EgQN",
        "outputId": "61b48ad3-9797-4f0e-fabc-c2baa59c1e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your task is to act as a Python Code Explainer.\n",
            "I'll give you a Code Snippet.\n",
            "Your job is to explain the Code Snippet Step-by-Step.\n",
            "Also, compute the final output of the code.\n",
            "Code Snippet is shared below, delimited with triple backticks:\n",
            "\n",
            "```\n",
            "\n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "nldXvoQ3FNWd",
        "outputId": "993a8442-e1e2-4a50-d5f9-3f064130e768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-Step:\n",
            "1. The function `my_func` takes an integer as input and returns `\"High\"` if the input is greater than 5, otherwise returns `\"Low\"`.\n",
            "2. The variable `result` is assigned the value of `my_func(4)` + `my_func(6)` + `my_func(4)`.\n",
            "3. The `print` statement prints the value of `result` to the console.\n",
            "\n",
            "Output:\n",
            "```\n",
            "LowHighLow\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Effective Prompting**"
      ],
      "metadata": {
        "id": "sqQGl7qQFgi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "  global x\n",
        "  x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lL4f0x46FkD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xudiEApxG0Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "  ```\n",
        "  {input_code}\n",
        "  ```\n",
        "  \"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYd5UilqG-rH",
        "outputId": "249cd69a-4492-497e-e388-309cffefce8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Your Task is to act as Python Code Explainer.\n",
            "  I'll give you a Code Snippet.\n",
            "  Your Job is to explain the Code Snippet step-by-step.\n",
            "  Break down the code into as many steps as possible.\n",
            "  Share intermediate checkpoints & steps along with results.\n",
            "  Few good examples of python code output between #### seperator:\n",
            "  ####\n",
            "  \n",
            "----------------------------\n",
            "Example 1: Code Snippet\n",
            "x = 10\n",
            "def foo():\n",
            "  global x\n",
            "  x = 5\n",
            "\n",
            "foo()\n",
            "print(x)\n",
            "Correct output: 5\n",
            "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
            "So, print(x) outside the function print the modfied value is 5.\n",
            "-----------------------------\n",
            "\n",
            "Example 2: Code Snippet\n",
            "def modify_list(input_list):\n",
            "  input_list.append(4)\n",
            "  input_list = [1,2,3]\n",
            "my_list = [0]\n",
            "modify_list(my_list)\n",
            "print(my_list)\n",
            "Correct output: [0, 4]\n",
            "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
            "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
            "So, print(my_list) outputs [0, 4].\n",
            "------------------------------\n",
            "\n",
            "  ####\n",
            "  Code Snippet is shared below, delimited with triple backticks:\n",
            "  ```\n",
            "  \n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "  ```\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "SV-DwAGPHKG5",
        "outputId": "4529d261-8791-4398-b660-a674c152b4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####\n",
            "  Step 1: Create a function `my_func()` with an input parameter `x`.\n",
            "```\n",
            "def my_func(x):\n",
            "```\n",
            "\n",
            "Step 2: Check if `x` is greater than 5. If it is, return \"High\". Otherwise, return \"Low\".\n",
            "```\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "```\n",
            "\n",
            "Step 3: Create a variable `result` and assign it to the sum of the three calls to `my_func()` with the arguments 4, 6, and 4.\n",
            "```\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "```\n",
            "\n",
            "Step 4: Print the value of `result`.\n",
            "```\n",
            "print(result)\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "HighLowLow\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradia App**"
      ],
      "metadata": {
        "id": "D8r-Pt9U0_uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "N-sjjQs-1Q9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define completion function\n",
        "def get_completion(code_snippet):\n",
        "\n",
        "  python_code_example = f\"\"\"\n",
        "  ----------------------------\n",
        "  Example 1: Code Snippet\n",
        "  x = 10\n",
        "  def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "  foo()\n",
        "  print(x)\n",
        "  Correct output: 5\n",
        "  Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "  So, print(x) outside the function print the modfied value is 5.\n",
        "  -----------------------------\n",
        "\n",
        "  Example 2: Code Snippet\n",
        "  def modify_list(input_list):\n",
        "    input_list.append(4)\n",
        "    input_list = [1,2,3]\n",
        "  my_list = [0]\n",
        "  modify_list(my_list)\n",
        "  print(my_list)\n",
        "  Correct output: [0, 4]\n",
        "  Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "  Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "  So, print(my_list) outputs [0, 4].\n",
        "  ------------------------------\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Also, give me some test cases related to the code snippet.\n",
        "  Provide Youtube link explaining the code snippet.\n",
        "  End the conversation in a funny manner using a funfact.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "\n",
        "  {code_snippet}\n",
        "\n",
        "  \"\"\"\n",
        "  completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt,\n",
        "    temperature=1,\n",
        "    # The maximum length if the response\n",
        "    max_output_tokens=600,\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ],
      "metadata": {
        "id": "lxQnBKh21rp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradio.Interface(get_completion, \"text\", \"text\").launch(share=False)"
      ],
      "metadata": {
        "id": "PtonjN6XAny1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define app UI\n",
        "import gradio\n",
        "\n",
        "iface = gradio.Interface(fn=get_completion, inputs=[gradio.Textbox(label=\"Insert Code Snippet\")],\n",
        "                     outputs=[gradio.Textbox(label=\"Explanation Here\")],\n",
        "                     title=\"Code Sensai\")\n",
        "iface.launch(share=True,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "j_YDHUgZ485y",
        "outputId": "c56c92c9-b14a-4898-f607-7b9921c083ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1c19a6c0b39b1bf2a3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1c19a6c0b39b1bf2a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1c19a6c0b39b1bf2a3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "_9CmH0Cf6lKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGfqIvAxxeRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}